## Two cameras: the epipolar constraint is "two rays must intersect"

A calibrated pinhole camera observation of a point is a **ray** in 3D. With a second camera, two rays correspond to the same world point iff they intersect. In blade terms, "these two lines intersect" is "their join is degenerate," i.e. their outer product vanishes.

The chapter derives this by writing each camera as a rigid motion (rotation + translation) acting as an outermorphism. If camera centers are at $a$ and $b$, and the observed direction vectors are $x_A$ and $x_B$ in their local frames, the world rays have the form

* from camera A: $(e_0 + a)\wedge R_A[x_A]$,
* from camera B: $(e_0 + b)\wedge R_B[x_B]$,

and the intersection condition reduces (after stripping the homogeneous component) to the clean trivector equation (Figure 12.3, printed p.342):
$$
0 = R_A[x_A] \wedge (b-a) \wedge R_B[x_B].
$$
In the coordinate frame of camera A, this becomes
$$
0 = x_A \wedge t_A \wedge R_B^A[x_B],
$$
where $t_A$ is the translation from A to B expressed in A's frame and $R_B^A = R_A^{-1}R_B$ is the relative rotation.

## Dualize to recover the classical scalar/matrix form (essential matrix)

Taking the Euclidean dual turns the trivector constraint into the standard scalar epipolar equation:
$$
0 = x_A \cdot \big(t_A \times R_B^A[x_B]\big).
$$
Written as a bilinear form,
$$
0 = x_A^T\, [t_{A\times}]\, R_B^A\, x_B,
$$
and the product
$$
E = [t_{A\times}]\,R_B^A
$$
is exactly the **essential matrix**.

So: the "mysterious" essential matrix is just "dualize the trivector incidence statement."

## Lines in the image: you really observe a plane of rays

Point observations give rays (lines). Line observations give something slightly different: an observed **image line** corresponds to a **plane of rays** through the pinhole.

In a canonical camera at the origin, a line in the image plane at location $x$ with direction $u$ can be written as
$$
L = (e_0 + \mathbf f + x)\wedge u = e_0 u + M,
$$
with a Euclidean moment bivector
$$
M = (\mathbf f + x)\wedge u.
$$
The plane of rays generated by that image line is
$$
e_0 \wedge L = e_0 \wedge M,
$$
so it depends only on $M$ (Figure 12.4, printed p.343). This is the structural reason line-based stereo becomes clean in geometric algebra: you don't need to bolt Plücker machinery onto a point-only framework—lines are already native.

## Three cameras: trifocal constraints from "three ray planes meet in a line"

With three cameras A, B, C all observing the same world line, each observation gives a ray-plane in world coordinates. Those three planes should intersect in the common 3D line, meaning the meet is degenerate. Expanding that degeneracy and factoring gives the chapter's trifocal constraint (their Eq. 12.15) in a compact geometric form:
$$
0 = m'_A \wedge\Big( \big((c-a)\cdot m'_C\big)m'_B \;-\; \big((b-a)\cdot m'_B\big)m'_C \Big),
$$
where $m'_A,m'_B,m'_C$ are the (rotated-to-world) dual moment vectors of the observed ray-planes, and $a,b,c$ are the camera centers.

This is bilinear in the observed lines from two views and linear in the third, which is why it can be packaged as a **trifocal tensor** $T(m_B,m_C)$ that predicts $m_A$ up to scale. Fixing one observed line (say in C) collapses this to a homography-like transfer rule between the other two images—another classical result that, here, is just "factor the blade equation."

## From theory to a working reconstruction loop: markers in motion capture

The programming section shows the same geometry running inside an actual reconstruction pipeline (Figures 12.6–12.8, printed pp.351–354):

1. For a marker observation in camera 1, build the plane through the two camera centers and the observed image point/ray.
2. Intersect that plane with camera 2's image plane to get the **epipolar line** in image 2.
3. Search for observed 2D markers close to that epipolar line (distance computed as a contraction in the image plane's algebra).
4. For each plausible match, reconstruct a 3D point by taking the two 3D rays and computing their **closest points**, then averaging.
5. Cluster the reconstructed 3D candidates; use the homogeneous $e_0$ weight as a crude vote count for "how many cameras supported this."

### Closest points on two skew rays (the inner-loop primitive)

Given two lines in factored form $P_1 + d_1D_1$ and $P_2 + d_2D_2$, the chapter's GA implementation computes $(d_1,d_2)$ by projecting the difference vector onto the bivector span of the directions using **reciprocals**:

* form the bivector $I = D_1 \wedge D_2$ (if $I=0$, directions are parallel),
* invert $I$ to get $I^{-1}$,
* compute reciprocals $rD_1 = D_2 \,\lrcorner\, I^{-1}$ and $rD_2 = D_1 \,\lrcorner\, I^{-1}$,
* then
  $$
  d_1 = rD_1 \cdot (P_2-P_1), \qquad d_2 = rD_2 \cdot (P_2-P_1).
  $$

That's the exact same story the chapter has been telling from the start: once you keep the blade structure, "hard" 3D geometry operations become short, stable algebra—good enough to sit in the hot loop of real code.
