# Chapter 22: Specializing the Structure for Efficiency

This chapter describes an approach to efficiently implementing geometric algebra. By "efficient" we mean that if you write a program that uses geometric algebra to do some geometry, it should be about as fast as a similar program that uses a traditional optimized method to do the same geometry. For example, the homogeneous model in geometric algebra (Chapter 11) should perform on par with a traditional implementation of homogeneous coordinates in linear algebra. We will demonstrate that this is possible with benchmarks of a ray tracer. The ray tracer itself is described in the next chapter.

We assume that you have read the preceding implementation chapters, as our goal here is to implement efficiently what is described there. The approach we take is largely independent of a specific programming language, as long as the language has some concepts of object orientation (i.e., classes). Some examples of actual C++ code generated by `Gaigen 2` are given to make the approach more tangible.

---

## 22.1 Issues in Efficient Implementation

By now, we hope we have convinced you that geometric algebra is a structured and elegant mathematical framework for geometry. In computer science, better structure and greater elegance often help to simplify the implementation of problems and to avoid errors.

However, when geometric algebra is first presented to a computer scientist or programmer, the question arises how an implementation of geometric algebra can ever be as efficient as the current way of implementing geometry (based in linear algebra with various extensions). Many features of geometric algebra seem to work against efficiency: $2^n$ coordinates are required to represent a multivector for an $n$-dimensional space, the conformal model of 3-D space requires a 5-D algebra (with its 32-D basis), there appear to be many products and operations, and so on.

All initial geometric algebra implementations (like [43, 47]) seemed to confirm this, for they were structurally pretty but unacceptably slow for practical applications on high volume data. They were not designed with efficiency in mind. This is a pity, for the structure of geometric algebra actually offers many opportunities to home in on the essential computations that need to be done in an application. Effectively, these permit its practical use as a high-level programming language that computes efficiently.

Three issues will recur in the attempt to make an efficient implementation: multivectors, metrics, and operations.

- **Multivectors** are the fundamental elements of computation in geometric algebra, but they are big ($2^n$ coordinates for $n$-D). So although it is mathematically attractive that all elements can be considered as multivectors, you would rather not base an implementation on it.

  Fortunately, most geometrically sensible elements of computation use only limited grades, and in a particular manner. This suggests defining a fixed, specialized multivector type for a variable whenever possible, to reduce storage and processing. Such a multivector type is an indication of its geometrical meaning, immediately implying a more limited use of grades or basis blades. Examples of specialized multivector types in the programming examples of Part I and II abound: `vector`, `translator`, and `circle`, to name a few.

  In a large group of geometric algorithms and applications, we can naturally impose such fixed multivector types on variables without compromising our solutions. (And if in some application we cannot, we can always revert to the multivectors.)

- **The metric** is a very fundamental feature of a geometric algebra, affecting the most basic products. Many different metrics are useful and need to be allowed, yet looking up the metric at run-time (e.g., from a table) is too costly. We should therefore not just implement a general geometric algebra and use it in a particular situation; for efficiency, we need a special implementation for every metrically different geometric algebra.

- **The number of basic operations** on multivectors is quite large, and every operation can be applied on every element. Written out in coordinates, the operations are not complicated, but precisely because the execution of each individual product or operation requires relatively few computations, any overhead imposed by the implementation (such as a conditional branch due to looping) will result in a significant degradation of performance.

  As a consequence, expressions consisting of multiple products and operations can often be executed much more efficiently by folding them into one calculation, rather than executing them one by one through a series of generic function calls.

  This suggests programming things out explicitly for each operation and each multivector type, with the danger of a combinatorial explosion in the volume of the code.

These considerations lead to the conclusion that for efficient performance, the elegant universal and unifying structure of geometric algebra needs to be broken up. Multivectors are general but too big, metrics are unifying but too basic to be variables, and the operations are simple and universal but too slow when not specialized and optimized.

Building such an optimized implementation of a particular geometric algebra is a lot of administrative work. It may be possible to do it by hand in limited, low-dimensional cases, but even then it is tedious and error-prone.

Fortunately, we do not need to do it ourselves. We should use the actual algebra to convert its coordinate-free expressions into low-level, coordinate-based code that can be directly executed by a processor. Used in that way, geometric algebra can be the engine of an *automatic code generator*, which can take a high-level specification of a solution for a geometric problem and automatically generate efficient implementations.

With such a tool, we never again have to write the error-prone, low-level code that directly manipulates coordinates. Instead, we just write our algorithms in the high-level language, confident that this will automatically lead to correct and efficient code.

---

## 22.2 Generative Programming

Generative programming [11] is the craft of developing programs that synthesize other programs. One of its uses is to connect software components (such as algebra implementations) without loss of performance. Ideally, each component adapts dynamically to its context. In our case, we would like to use generative programming to transform the specification of a geometric algebra into an optimized implementation that is tailored to the needs of the rest of the program.

There are several points in the tool-chain (illustrated in Figure 22.1) from source code to fully linked and running program where this transformation can take place. Here we list three obvious examples, but other approaches (or hybrids) are also possible:

**Figure 22.1:** Basic tool-chain from source code to running application with three possible points where code generation can take place:

$$\text{Source code} \rightarrow \underset{(1)}{\text{Compile}} \rightarrow \underset{(2)}{\text{Link}} \rightarrow \underset{(3)}{\text{Run}}$$

1. **Pre-compilation code generation.** The most explicit approach is generating an implementation of the algebra before the actual compilation of the program takes place. This is the way classical code generators like `lex` and `yacc` work. Advantages are that generated code is directly available to the user as a library, and that this method does not cause interference with the rest of the chain. The main disadvantage is that it does not integrate well, since a separate program is required to generate the code. Another disadvantage is that some form of feedback from the final, running program (i.e., profiling) is required to allow the implementation to adapt itself to the context, and this may require an extra code generation pass. The geometric algebra implementations `Gaigen` and `Gaigen 2` take this approach.

2. **Compile-time meta-programming.** The transformation can also take place at compile-time, if the programming language permits this. This is called meta-programming, where the algebra implementation is set up such that the compiler generates parts of it at compile time. For example, in C++ this is possible through the use of the *templates* feature that was originally added to the language for generic programming. The definite advantage of this method is the good integration with the language. A disadvantage is the limited number of mainstream programming languages that support meta-programming. Disadvantages specific to the C++ programming language are the complicated template syntax, hard-to-decipher compiler error messages (for both user of the library and its developer), and the long compile times. The `boost::math::clifford` library [61] uses this approach.

3. **Run-time code generation.** A third option is to delay code generation until the program is up and running. The program would generate the code as soon as it is required (e.g., the first time a function of the algebra is called with specific arguments). The advantages are that the algebra implementation can adapt itself to the actual input and the actual hardware that it runs on (e.g., available instruction set extensions, the speed of registers compared to cache compared to main memory, etc. [5]). Disadvantages are the slower startup time of the final program (due to the run-time code generation) and the fact that the method is rather nonconventional and hard to implement. At the time of writing, we are not aware of such a geometric algebra implementation.

The geometric algebra implementation described below can in principle be realized through each of these generative programming methods, so below we simply call it the *code generator*, regardless of the precise details.

---

## 22.3 Resolving the Issues

The overall goals of our implementation can be stated as follows:

1. Waste as little memory as possible (i.e., store each variable as compactly as possible).

2. Implement functions over the algebra most efficiently. To do this you need several things:
   - (a) Process as few zero coordinates as possible (which coincides with Goal 1),
   - (b) Minimize (unpredictable) memory access. This coincides with Goal 1 and Goal 2a, but also demands avoiding lookup from tables, and so on.
   - (c) Avoid conditional branches. When a modern processor mispredicts a conditional branch, a large number of processor cycles is lost.
   - (d) Unroll loops (e.g., over the dimension of the algebra or the grade of a blade) whenever possible. This avoids branches and also allows you to apply further optimizations.
   - (e) Optimize nontrivial expressions. Avoid implementing them as a series of function calls.

When we hold our (purposely naive) reference implementation based on Chapters 19 to 21 up to this list of demands, we see that it violates all but one of them. It wastes memory due to the bookkeeping required for per-coordinate compression, looks up the metric from tables, uses many conditional branches due to looping, does not unroll loops, and processes operations one by one. The only thing that it does right is minimizing the processing of zero coordinates.

### 22.3.1 The Approach

Through generative programming, we can resolve each of the issues listed in Section 22.1 in a way that satisfies these goals. We first sketch how the issues can be resolved, followed by a more detailed description in the next section.

- As we indicated, the problem that multivectors are too general can be resolved by generating classes for specific multivector types. These classes store only the nonzero coordinates for that specific type (e.g., the 3-D vector class only stores the $\mathbf{e}_1$-, $\mathbf{e}_2$- and $\mathbf{e}_3$-coordinates, as all other coordinates are always 0). We can also generate classes that represent useful constants (such as $\mathbf{e}_1$, $\mathbf{I}$, and $o \wedge \infty$). These classes are used mostly as symbolic tokens, as they will be optimized away when they are used in expressions. The combination of these two ideas leads to a hybrid where only some coordinates are constant. For example, the $o$-coordinate of a normalized homogeneous point is always 1.

- Using generative programming, the metric of the algebra can now be hard-coded directly into the implementation by the code generator, taking very little effort from the user. Likewise, the large implementation effort due to the combinatorial explosion caused by the large number of products/operations and multivector types is no longer an issue, as the code generator does the tedious work for us (note that not every function should be generated for every combination of arguments, but only those that are actually used by the program).

- Finally, by optimizing functions over the algebra, the code generator can fully optimize equations consisting of multiple products and operations, because it can oversee the whole context.

The approach is built on top of Chapters 19 and 20. First, it can be seen as an optimization of the list of basis blades approach from Section 20.2. The optimization is to generate a separate class for each type of list (i.e., multivector type) that we expect to encounter in our program, thus avoiding the need for explicit bookkeeping of the list. Second, to write out expressions (involving products and operations) on a basis, we need Chapter 19. The nonlinear functions from Chapter 21 are not essential to the approach, but most of them are suitable candidates for optimization, as described in Section 22.4.6.

---

## 22.4 Implementation

We now describe the implementation in some more detail, starting with the specification of the algebra, followed by a description of the classes and functions that should be generated.

### 22.4.1 Algebra Specification

The specification of the algebra is the starting point for the code generator. It specifies at least the following:

- The dimension of the algebra;
- The metric of the algebra;
- Definition of the specialized types;
- Definition of constants.

We briefly discuss these terms before moving on to their implementation.

#### Dimension of the Algebra

The geometric algebra of $\mathbb{R}^n$ has the dimensionality $n$, which should be specified. Within $\mathbb{R}^n$ it is convenient to specify a basis (not necessarily orthonormal; see below). Since geometric algebra is too new to have a standardized notation, it is sensible to permit the user to choose the names of the basis vectors. This results in more readable code, and output in a recognizable format.

#### Metric

The metric should allow for a nondiagonal metric matrix, even though an equivalent metric with a diagonal metric matrix always exists. The reason for this is that in certain models, it may be more intuitive and efficient to define the metric in an off-diagonal manner. For example, in the conformal model of Euclidean geometry, $\infty$ and $o$ are more fundamental than the vectors $e$ and $\bar{e}$ (see (13.5) for the relationships of these bases).

The increased efficiency of this basis is obvious when we write out the coordinates of a conformal line $A$ relative to each basis. Using the $o, \infty$-basis we get

$$A = A_{1o\infty} \mathbf{e}_{1o\infty} + A_{2o\infty} \mathbf{e}_{2o\infty} + A_{3o\infty} \mathbf{e}_{3o\infty} + A_{12\infty} \mathbf{e}_{12\infty} + A_{13\infty} \mathbf{e}_{13\infty} + A_{23\infty} \mathbf{e}_{23\infty},$$

while the $e, \bar{e}$-basis results in

$$A = A_{1e\bar{e}} \mathbf{e}_{1e\bar{e}} + A_{2e\bar{e}} \mathbf{e}_{2e\bar{e}} + A_{3e\bar{e}} \mathbf{e}_{3e\bar{e}} + A_{12\bar{e}} \mathbf{e}_{12\bar{e}} - A_{12e} \mathbf{e}_{12e} + A_{13\bar{e}} \mathbf{e}_{13\bar{e}} - A_{13e} \mathbf{e}_{13e} + A_{23\bar{e}} \mathbf{e}_{23\bar{e}} - A_{23e} \mathbf{e}_{23e}$$

(here we use shorthand to denote the coordinates $A_{...}$ and the basis elements $\mathbf{e}_{...}$, i.e., $\mathbf{e}_{13\infty} = \mathbf{e}_1 \wedge \mathbf{e}_3 \wedge \mathbf{e}_\infty$, etc.). So, storing a line requires six coordinates on the $o, \infty$-basis compared to nine coordinates on the $e, \bar{e}$-basis, three of which are duplicates ($A_{12\bar{e}} = A_{12e}$, $A_{13\bar{e}} = A_{13e}$, and $A_{23\bar{e}} = A_{23e}$).

#### Specialized Types

The definition of each specialized type should list the basis blades required to represent it. The definition could also include whether the type is a blade or a versor, so that run-time checks can be made (e.g., in debug mode) to verify the validity of the value.

Preferably, the definitions should also allow certain coordinates to be defined as constant. For example, a normalized point has a constant $o$-coordinate, so there is no need to store it. It may also be useful to have the ability to specify the order in which coordinates are stored. This can simplify and optimize connections to other libraries, which may expect coordinates to be in a certain order.

Specialized types for matrix representations of outermorphisms are also an option. For example, in connection with OpenGL it is useful to have a special type to contain the outermorphism matrix representation that can be applied directly to a grade 1 blade (a $4 \times 4$ matrix). In our programming examples, we have used such a type in Sections 11.13.2, 12.5.1, and 13.10.3.

#### Constants

Important constant multivector values inside a program can be encoded as constants. Examples are basis vectors such as $\mathbf{e}_1$ and $o$ or the pseudoscalar $\mathbf{I}$. We should include the constants in the algebra specification and generate special types for them, in such a way that their use in expressions can be optimized away by the code generator or the compiler.

### 22.4.2 Implementation of the General Multivector Class

In our approach, an implementation of the general (nonspecialized) multivector is always required. The specialized types approach works best when the multivector type of each variable in a program is fixed. This assumption often holds, but also cripples one of the amazing abilities of geometric algebra: many equations work regardless of the multivector type of their input. For some programs (e.g., those dealing with noisy input or interactive input), the multivector type of variables may change at run-time. This prevents the use of specialized multivector types at compile-time, and hence for such programs we should provide a fallback option that may not be efficient, but at least works.

#### Coordinate Compression

An important issue when implementing the general multivector class is compression of the coordinates. The coordinates of the general multivector could be stored naively (all $2^n$ of them), but to increase performance, compression can be used to profit from the inherent sparseness of geometric algebra. By compression, we mean reducing the number of zero coordinates that are stored.

- **Per-Coordinate Compression.** The most straightforward compression method is per-coordinate compression. Each coordinate is tagged with the basis blade it refers to, and the coordinates are stored in a list. Zero coordinates are not stored in this list. We already encountered this type of compression in Section 20.2.

- **Per-Grade Compression.** Multivectors are typically sparse in a per-grade fashion: blades, by definition, have only one nonzero grade part. Likewise, versors are either even or odd, implying that at least half their grade parts are null. This suggests grouping coordinates that belong to the same grade part: when a grade part is not used by a multivector, the entire group of coordinates for that grade is not stored. Instead of signaling the presence of each individual coordinate, we signal the presence of the entire group.

  For low-dimensional spaces, per-grade compression is a good balance between per-coordinate compression and no compression at all. Per-coordinate compression is slow because each coordinate has to be processed individually, leading to excessive conditional branching, which slows down modern processors. No compression at all is slow because lots of zero coordinates are processed needlessly. Per-grade compression reduces the number of zero coordinates while still allowing sizable groups of coordinates to be processed without conditional branching.

- **Per-Group Compression.** Per-grade compression does miss some significant compression opportunities. For example, a flat point in the conformal model (a 2-blade) requires only four coordinates, while the grade 2 part of 5-D multivectors requires 10 coordinates in general. Thus per-grade compression stores at least six zero coordinates for each flat point. As the dimension and structure of the algebra increases, this storage of zero coordinates becomes more of an issue. A more refined grouping method may alleviate this problem, but what the groups are depends heavily on the specific use of the algebra. For example, in the conformal model for Euclidean geometry, one could group coordinates based on the presence of $\infty$ in the basis blades, in addition to grouping them based on grade; but if the conformal model would be used for hyperbolic geometry (see Section 16.7.1), a grouping based on the presence of $e$ would be better.

#### Example of a Multivector Class

In C++, the general multivector class that uses per-grade compression could look like:

```cpp
// C++ code:
class multivector {
public:
    /* ... constructors, compression functions, pretty printing,
       etc ... */

    /**
     * Bitmap that keeps track of grade usage (gu).
     * When bit 'i' of 'gu' is 1, then grade 'i' is present in the
     * coordinates.
     */
    unsigned int gu;

    /** dynamically allocated memory that holds coordinates */
    double *c;
};
```

### 22.4.3 Implementation of the Specialized Multivector Classes

Specialized multivector classes should be generated for each multivector type used in the program. The implementation of these classes can be straightforward. The class should provide storage for the nonconstant coordinates and some functionality is required to convert back and forth between the specialized multivectors classes and the general multivector class. The rest of the functionality is provided by the functions over the algebra.

As an example in C++, the specialized multivector class for a normalized flat point in the conformal model for Euclidean geometry could look like:

```cpp
// C++ code:
class normalizedFlatPoint {
public:
    /* ... constructors, converters, pretty printing, etc ... */

    /** the no^ni coordinate is constant: */
    static const double noni = 1.0;

    /** holds the e1^ni, e2^ni and e3^ni coordinates: */
    double c[3];
};
```

### 22.4.4 Optimizing Functions Over the Algebra

Optimizing functions over elements of the algebra is the key to achieving high performance. As described so far, we only have classes that can efficiently store general and specialized multivectors. Optimized functions over these algebra elements should also be generated to do something with the multivectors. Preferably, we generate these functions from their high-level definitions.

An example of the definition of such a function (in an imaginary language) is:

```
// code in imaginary language:
// applies normalized versor 'V' to multivector 'X'
function multivector applyVersor(multivector V, multivector X)
{
    return V X reverse(V);
}
```

Such a function would be used to apply any versor to a blade or another versor. The precise syntax of the definition depends on the generative programming method used. For example, in the meta-programming approach, they appear as a C++ template functions, while `Gaigen 2` uses a domain-specific language to define them.

The point is that such functions should be instantiated with specific types of multivectors. For example, when `applyVersor()` is called with `rotor` and `flatPoint` arguments, the code generator should take the `applyVersor()` function definition and specialize it for those arguments. Figure 22.2 shows the resulting C++ code generated by `Gaigen 2`, which should serve as a convincing example that this type of code should indeed be automatically generated and not written by hand.

**Figure 22.2:** Code generated by `Gaigen 2`. The function `applyVersor()` (see text) was instantiated with a `rotor` and a `flatPoint`. Both variables `V` and `X` contain an array of floating-point coordinates named `c`. Note that `Gaigen 2` does not know that the rotor is normalized, hence the needless computations in the last line of code, which always results in '1' for such normalized versors.

```cpp
// generated C++ code:
flatPoint applyVersor(const rotor& V, const normalizedFlatPoint& X) {
    return flatPoint(
        // e1 ^ ni coordinate:
        2.0 * (
            V.c[0] * X.c[0] * V.c[0] - V.c[1] * X.c[0] * V.c[1] +
            V.c[1] * X.c[1] * V.c[0] + V.c[3] * X.c[1] * V.c[2] -
            V.c[0] * X.c[2] * V.c[3] + V.c[2] * X.c[2] * V.c[1])
        , // e2 ^ ni coordinate:
        2.0 * (
            V.c[3] * X.c[0] * V.c[2] - V.c[1] * X.c[0] * V.c[0] +
            V.c[0] * X.c[1] * V.c[0] - V.c[1] * X.c[1] * V.c[1] +
            V.c[2] * X.c[2] * V.c[0] + V.c[3] * X.c[2] * V.c[1])
        , // e3 ^ ni coordinate:
        2.0 * (
            V.c[0] * X.c[0] * V.c[3] + V.c[2] * X.c[0] * V.c[1] +
            V.c[3] * X.c[1] * V.c[1] - V.c[2] * X.c[1] * V.c[0] +
            V.c[1] * X.c[2] * V.c[1] - V.c[3] * X.c[2] * V.c[3])
        , // no ^ ni coordinate:
        V.c[0] * V.c[0] + V.c[1] * V.c[1] + V.c[2] * V.c[2] + V.c[3] * V.c[3]
    );
}
```

The process of generating code like Figure 22.2 from the high-level definition of the function like `applyVersor()` is as follows:

1. The types of the arguments are replaced with specializations.
2. The expressions in the function are written out on a basis.
3. The expressions are simplified symbolically: products and operations are executed at the basis level (Chapter 19), identical terms are added, unnecessary computations removed, and so on.
4. The return type of the function is determined.
5. The code is emitted.

This process is similar to how one would go about when writing such an optimized function by hand. As a result of the design (i.e., the use of specialized multivectors instead of other methods of compression) and code generation approach, the code in Figure 22.2 contains no conditional branches. These branches would otherwise be present due to coordinate compression and/or looping.

To illustrate the use of constants and constant coordinates, we present two more examples. Again in the conformal model of Euclidean geometry, let us instantiate the following function with a normalized flat point and a dual plane as arguments:

```
// code in imaginary language:
function multivector outerProduct(multivector a, multivector b)
{
    return a ^ b;
}
```

The $o$-coordinate of the normalized flat point is a constant 1. We can specialize our outer product code for when the operands are a flat point and a dual plane, which reduces the necessary computation by about 25 percent, as we have indicated in the following code generated by `Gaigen 2`:

```cpp
// generated C++ code:
line outerProduct(const normalizedFlatPoint& x, const dualPlane& y)
{
    return line(
        x.c[1] * y.c[0] - x.c[0] * y.c[1],
        x.c[2] * y.c[0] - x.c[0] * y.c[2],
        x.c[2] * y.c[1] - x.c[1] * y.c[2],
        y.c[0],    // thanks to normalized flat point,
        y.c[1],    // no 'x' coordinates are used
        y.c[2]);   // on these lines
}
```

The constant coordinates save three multiplies in the last three lines of the function. Also, compare this optimized function to its (generic) equivalent from the reference implementation (Figure 20.2). The optimized function contains no loops or conditionals, which can make a difference of as much as two orders of magnitude in performance.

As a final example, let us instantiate a function that uses a constant (the unit pseudoscalar $\mathbf{I}$) to dualize a multivector in the vector space model:

```
// code in imaginary language:
function multivector dual(multivector a)
{
    return a . inverse(I);
}
```

We instantiate this function using an ordinary vector. Because $\mathbf{I}$ is a constant, the function reduces to initializing a bivector with the (shuffled, negated) coordinates of the `a`:

```cpp
// generated C++ code:
bivector dual(const vector& a)
{
    return bivector(-a.c[2], -a.c[0], -a.c[1]);
}
```

The inversion and the inner product have disappeared after this automatic optimization by the code generator. Compare this to the reference implementation, which would explicitly compute the inverse of $\mathbf{I}$ and then perform the actual inner product, only to achieve the same final result.

### 22.4.5 Outermorphisms

When an outermorphism has to be applied many times, it is often more efficient to compute a matrix representation of the outermorphism and use the matrix to apply the outermorphism.

Consider a situation where a lot of points have to be rotated. We could do this using a rotor $R$ directly ($R X \tilde{R}$) by calling the function in Figure 22.2. However, because $R X \tilde{R}$ is an outermorphism, the same effect can be achieved by initializing an outermorphism matrix (see Section 4.2) and then using this matrix to transform the points. The transform of points using such a matrix can be done using the automatically generated code in Figure 22.3, which is obviously more efficient.

**Figure 22.3:** Code generated for transforming a flat homogeneous point according to a $4 \times 4$ matrix. Note that the flat point has a constant `ni^no` coordinate, which saves four multiplies. Also note that the return type is deduced to be a `flatPoint` instead of a `normalizedFlatPoint`, because the outermorphism may undo the normalization of the point.

```cpp
// generated C++ code:
flatPoint apply_om(const omFlatPoint& M, const normalizedFlatPoint& a)
{
    return flatPoint(
        M.c[0] * a.c[0] + M.c[1] * a.c[1] + M.c[2] * a.c[2] + M.c[3],
        M.c[4] * a.c[0] + M.c[5] * a.c[1] + M.c[6] * a.c[2] + M.c[7],
        M.c[8] * a.c[0] + M.c[9] * a.c[1] + M.c[10] * a.c[2] + M.c[11],
        M.c[12] * a.c[0] + M.c[13] * a.c[1] + M.c[14] * a.c[2] + M.c[15]);
}
```

The code in Figure 22.3 was generated from the following definition:

```
// code in imaginary language:
function multivector apply_om(outermorphism M, multivector a)
{
    return M * a;
}
```

Whether it is more efficient to use the matrix representation or apply versors directly depends on how much time it takes to initialize the matrix, how often you are going to apply it, and how much more efficient the matrix is compared to the straightforward geometric algebra implementation.

### 22.4.6 Optimizing the Nonlinear Functions

One might wonder if it is possible to use these optimization techniques to generate efficient implementations of the nonlinear functions in Chapter 21. The answer depends on whether the multivector types of intermediate and output variables of the algorithms are fixed for a specific instantiation of the algorithm. For most of the algorithms, it can be done, and leads to significant performance gains.

Each algorithm from Chapter 21 should be defined by a function that---in order to generate an optimized version of it---should be instantiated with specialized multivector argument(s). For some of the algorithms (specifically multivector classification and factorization) it is necessary to unroll loops to make sure that each variable has a fixed multivector type. For example, the variable $\mathbf{B}_c$ in the blade factorization algorithm (Section 21.6) changes type on each iteration, but by unrolling the loop this can be avoided (assuming that a new variable $\mathbf{B}_c$ is introduced for each iteration of the loop).

We briefly discuss how optimization would proceed for each nonlinear function from Chapter 21:

- **Inverse of versors (and blades).** This is a simple equation that is very suitable for optimization.

- **Inverse of multivectors.** The initialization of the geometric product matrix can be done more efficiently because it is known that many coordinates are zero. The rest of the algorithm does not benefit from the technique described here. The return type of the function cannot be determined at code generation time, since it depends on the outcome of the matrix inversion, which is not predictable based solely on the specialized multivector type of the input argument.

- **Exponential, sine, and cosine of multivectors.** These functions can be highly optimized when the code generator is able to determine that the square of the specialized multivector argument is a scalar. Then the straightforward equations for the special cases can be used (see Section 21.3). Otherwise, the test for whether $\mathbf{A}^2$ is a scalar should be performed at run-time, and the outcome decides whether the special cases or the generic series evaluation algorithm should be used (neither of which can be now optimized).

- **Multivector classification.** This algorithm can be highly optimized. The first step (testing if the versor inverse is the true inverse) is straightforward. To optimize the second step (testing for grade preservation), the loop over all basis vectors should be unrolled. Then each test can be optimized for the individual basis vectors.

- **Blade factorization.** Optimizing blade factorization is similar to the multivector classification. As soon as the code generator is able to unroll loops, all variables in algorithm get a fixed type, and most conditional branches are removed (finding the largest basis blade of the blade and its basis vectors must be done at run-time, which requires some looping).

- **`meet` and `join` of blades.** The `meet` and `join` algorithms cannot be optimized by using specialized multivectors, because the multivector type of several variables in the algorithm is not fixed. For example, the type of the outcome of the delta product is by definition not predictable from just the multivector type of the arguments (the actual values of the arguments have to be known). There are several other unavoidable conditional branches in the algorithm.

---

## 22.5 Benchmarks

To get an idea of the relative performance of each model of 3-D Euclidean geometry and implementations thereof, we have written multiple implementations of a ray tracer. The code for each implementation is basically the same, except each time we use a different model of the Euclidean geometry. This allows us to measure the relative performance of the models in a realistic context.

The models we used are:

- **3D LA.** The vector space model implemented with 3-D linear algebra. $3 \times 3$ matrices are used to represent rotations.

- **3D GA.** The vector space model implemented with 3-D geometric algebra. Rotors are used to represent rotations.

- **3D GA-OM.** The above, but using outermorphism matrices to represent rotations.

- **4D LA.** Homogeneous coordinates, with Plucker coordinates for lines and planes. $4 \times 4$ matrices are used to implement rotations, scaling, and translations.

- **4D GA.** The homogeneous model. Outermorphism matrices are used to implement rotations, scaling, and translations.

- **5D GA.** The conformal model. All transformations implemented using versors. This version of the ray tracer is described in detail in Chapter 23.

The geometric algebra models were implemented using `Gaigen 2`, which is basically an implementation of the ideas in this chapter (the GA sandbox source code package is also based on it). The linear algebra models use typical handwritten libraries (all code in-lined as much as possible). SIMD instructions (i.e., Intel's SSE instruction set) were used only as a result of optimizations performed by the Intel C++ compiler. You can download the source of each version of the ray tracer at http://www.geometricalgebra.net.

**Table 22.1:** Performance benchmarks run on a Pentium 4, 3GHz notebook, with 1GB memory, running Windows XP. Programs were compiled using Intel Compiler 9.0.

| Model    | Implementation | Rendering Time Relative to 3D LA |
|----------|----------------|----------------------------------|
| 3D LA    | Standard       | 1.00x                            |
| 4D LA    | Standard       | 1.22x                            |
| 3D GA-OM | Gaigen 2       | 0.98x                            |
| 3D GA    | Gaigen 2       | 1.05x                            |
| 4D GA    | Gaigen 2       | 1.2x                             |
| 5D GA    | Gaigen 2       | 1.26x                            |

Table 22.1 shows the relative performance of the models. The 3D GA-OM model is most efficient by a small margin. This slight performance edge is due to the use of constants where the 3D LA model uses nonconstant vectors. The cost of using the conformal model 5D GA (about 25 percent) is approximately the same as the cost of using the homogeneous model 4D GA or homogeneous coordinates 4D LA.

---

## 22.6 A Small Price to Pay

In this chapter we presented a method that brings the performance of geometry implemented through geometric algebra close to that of geometry implemented through linear algebra. The core ideas of the method are specialization of multivectors and optimization of functions, implemented through generative programming. The benchmarks for the ray tracer example show that for the vector space model, performance is equal to that of linear algebra. In our benchmarks, the cost of using the conformal model is a drop of about 25 percent in performance, similar to using the customary homogeneous coordinates in such an application.

Of course, your mileage may vary. As described in detail in Chapter 23, the ray tracer was designed with performance in mind. When multiple representations were available for concepts (such as a ray), the alternative that performed the best was selected even though that representation may not have been the most elegant one. We have also seen applications where using the conformal model was *more* efficient than the traditional approach (40 percent in the inverse kinematics of a simple robot [34]). Different choices and different applications may perform better or worse, but we don't believe any well-designed conformal model application should be more than 50 percent slower than the best traditional implementation.

Besides achieving good performance, specializing multivectors also has the advantage that it makes your source code more readable. Instead of every geometric variable being of the `multivector` type, variables can have more informative types such as `line`, `circle`, and `tangentVector`, as can be seen in many programming examples in Part I and Part II. The disadvantage, of course, is that you lose the ability to store other types of multivectors in those variables.

The bottom line is that geometric algebra is competitive with classical approaches in computation speed, with the benefit of more readable high-level code and error-free low-level code.

---

## 22.7 Exercises

1. The weighted sum of which basis blades is required to represent a plane in general position on the $o, \infty$-basis? Which on the $e, \bar{e}$-basis?

2. What is the minimum number of coordinates required to represent any rigid body motion rotor versor in the conformal model?

3. Is there a difference in the basis for rotors and versors?

4. Write out the following equation in terms of coordinates: $\mathbf{n} = (\mathbf{a} \wedge \mathbf{b})^*$.
