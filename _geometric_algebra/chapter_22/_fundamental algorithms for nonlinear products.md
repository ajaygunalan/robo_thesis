# Fundamental Algorithms for Nonlinear Products

Nonlinear operations in geometric algebra are where the representation stops being "just linear algebra in disguise" and starts demanding algorithms that respect *structure* instead of coefficients. Linear products let you push multivectors through matrices or double loops; nonlinear ones—like inversion, exponentiation, meet/join, and factorization—force you to ask: *what kind of object is this multivector, really, and what hidden constraints does it satisfy?*

The chapter's through-line is that the fastest, most reliable algorithms appear the moment you exploit a strong algebraic promise. If you know you're holding a versor, inversion collapses to a scalar normalization—basically a one-liner—because of the scalar nature of $V\tilde V$; that's the entire point of [[versor inverse]]. If you *don't* have that promise, you can still invert by brute force via the left-multiplication matrix, but you pay in speed and numerical fragility, as [[multivector inversion via geometric product matrix]] makes painfully clear.

The same "promise vs. brute force" split shows up for transcendental functions. When $A^2$ is scalar, $\exp(A)$ is a clean trig/hyperbolic expression; when it isn't (common for bivectors beyond 3D, and especially in conformal GA), you fall back to truncated series, and then you fight floating-point with rescaling and repeated squaring—see [[exponential and scaling for multivectors]]. The logarithm flips the story: lots of useful closed forms exist in specific motion models, but there's no one universal "log of a versor" hammer.

Once you start doing repeated computations, a more subtle problem surfaces: numerical drift can turn "should be a blade/versor" into "close, but not quite." That's why classification is not cosmetic; it's infrastructure. [[multivector classification blade vs versor]] gives a structural test that doesn't get fooled by mixed-grade cancellations, and it separates the metric-free question "is this factorizable as an outer product?" from the metric-dependent question "is this a product of invertible vectors?"

Factorization is the next hinge: many downstream routines want *vector factors*, not a dense coordinate soup. The projection-based strategy in [[blade factorization by projection]] is the chapter's template example of *structure-aware extraction*: repeatedly project candidate vectors into the blade and peel off factors, but do it in a way that avoids wasting time on candidates that project to zero.

With factorization available, meet and join become computable in a way that mirrors set intersection/union, but with blades standing in for subspace generators. The key move is to use the highest-grade part of $AB$ as a "symmetric difference" blade (the delta product), which tells you the grades you're aiming for and supplies the factors you need to build the result incrementally. That whole pipeline lives in [[meet and join via delta product]].

Taken together, the chapter is basically a design rule for GA implementations: when an operation is nonlinear, don't start from coordinates—start from *invariants* (like scalar squares, grade behavior under sandwiching, and factor structure), and only drop to brute-force matrix methods when you truly have no better promise to exploit.
