# Chapter 7: Orthogonal Transformations as Versors — Q&A

---

## 7.1: What do you gain when "reflect any object" becomes a single sandwich product that works for vectors, planes, and higher subspaces alike?

Once reflection becomes a single sandwiching rule, you stop having separate "how do I reflect a vector?" / "how do I reflect a plane?" / "how do I reflect a k-dimensional subspace?" hacks. The same operator $a(\cdot)a^{-1}$ extends from vectors to blades (and then to multivectors by linearity), so reflections become a *universal* primitive: build it once, and it acts on anything you can represent in the algebra. That one move is the seed for the whole chapter, because rotations, composition, and versors are just "more sandwiches made from more reflections."

**Key equation:**
$$X' = aXa^{-1}$$

---

## 7.2: If every rotation is just an even number of reflections, how does that let you build and apply rotations in a way that scales to any dimension?

The point of "a rotation is an even number of reflections" is not abstract group theory—it's a direct computational recipe. If reflections are sandwiches, then two reflections collapse into one sandwich with a single element $R$, and you get a rotation that applies to *any grade* (vectors, bivectors/planes, etc.) with the same formula $X\mapsto RXR^{-1}$. That's the payoff: instead of building a special rotation mechanism for each geometric object type, you get one operator that rotates everything consistently.

**Key equation:**
$$X' = R X R^{-1}$$

---

## 7.2.1: Why is it powerful that a 3D rotation can be generated by the simple "ratio" rotor $R=b/a$ instead of a matrix full of coordinates?

The "ratio rotor" $R=b/a$ is the cleanest way to represent a 3D rotation because it's literally just "reflect in $a$, then reflect in $b$" written once and for all. You don't pick a coordinate frame, you don't assemble a matrix, and you don't care about the lengths of $a$ and $b$ because the inverse cancels scaling automatically. The only geometry that survives is what you actually want: the plane $a\wedge b$ and the relative angle between them.

**Key equation:**
$$x' = (b/a)\,x\,(b/a)^{-1}$$

---

## 7.2.2: How does $R x \tilde{R}$ automatically produce the "missing" perpendicular component needed for rotation without you constructing a basis or a cross product?

The "magic" of $R x \tilde{R}$ is that it manufactures the right perpendicular direction for you. You decompose $x$ relative to the rotation plane: the out-of-plane part $x^{\uparrow}$ stays fixed, the in-plane projection $x^{\parallel}$ gets scaled by $\cos\phi$, and the in-plane perpendicular $x^{\perp}$ appears automatically with $\sin\phi$. You didn't build a basis, you didn't compute a cross product, and you didn't manually construct $x^{\perp}$; the commutation rules with the plane bivector force the geometry to come out right.

**Key equation:**
$$R x \tilde{R} = x^{\uparrow} + \cos\phi\,x^{\parallel} + \sin\phi\,x^{\perp}$$

---

## 7.2.3: Why does the sign of a rotor encode the difference between a 2π and a 4π rotation for connected bodies (and why should you care if you build robots)?

Yes, $R$ and $-R$ produce the same rotated *vector* because the sandwich cancels the sign. But the sign still carries real information about the *path* of the rotation—whether you're effectively going the "long way around" (angles in $[0,4\pi)$) and what that means for connected bodies (the classic 4π/"plate trick" phenomenon). In practical terms: if you're interpolating orientations or controlling chained joints, that hidden "sense" can be the difference between smooth motion and accidentally winding your system up.

**Key equation:**
$$-R_I = R_{I(2\pi+\phi)}$$

---

## 7.3: Why is composing rotations literally just multiplying rotors $R_2R_1$, and what does that reveal about what the geometric product is *really* doing?

This is where rotors stop being "cute" and become a serious tool: composition is just multiplication. If you apply $R_1$ then $R_2$, associativity gives $(R_2R_1)x\widetilde{(R_2R_1)}$, so the combined motion is literally the product $R_2R_1$. That's the real meaning of the geometric product here: it's not "multiplying coordinates," it's multiplying operators.

**Key equation:**
$$R_{\text{total}} = R_2R_1$$

---

## 7.3.1: Why do planar rotations commute and collapse to a one-sided multiplication instead of needing the full sandwiching machinery?

In a single plane, rotation becomes almost embarrassingly simple: the sandwich collapses into one-sided multiplication by a "complex-like" factor. The reason is purely geometric: if $x$ lies in the plane $I$, it anticommutes with $I$, letting you slide the rotor across and fuse the half-angles into a full-angle factor. That's why planar rotations commute and why 2D feels like complex arithmetic—because, in that special case, it *is* the same structure showing up.

**Key equation:**
$$R_I x R_{-I} = x(\cos\phi + I\sin\phi)$$

---

## 7.3.2: If complex numbers are just vector ratios relative to a chosen "real axis," what geometric structure did the usual complex-number story quietly throw away?

Complex numbers look like "points in a plane" until you remember what they really encode: "how to rotate/scale a fixed real-axis vector into the target vector." In GA terms, a complex number is a *ratio* of vectors relative to a chosen axis, $A=a/e_1$, which quietly converts vectors into operators and breaks the plane's symmetry by privileging $e_1$. GA keeps both roles—vectors and rotors—available at once, so you can do geometry without losing track of what's an object and what's an action.

**Key equation:**
$$A = a/e_1 = a_1 - a_2 I$$

---

## 7.3.3: When you multiply two 3D rotations in different planes, where does the new rotation plane and angle come from—and can you read it directly from the product?

When rotation planes differ in 3D, their product doesn't stay in one plane—you get a new bivector that encodes the resulting plane, plus a scalar that encodes the resulting half-angle. The multiplication expands into a scalar part and a bivector part (no grade-4 in 3D), so the outcome is still "scalar + plane," just not the plane you started with. The practical win is that you don't need to guess the new axis/angle: the product *contains it*, and you can extract it by normalizing the bivector part and using an arctangent on the scalar vs bivector magnitudes.

**Key equation:**
$$(c'_t - I_t s'_t) = (c'_2 - I_2 s'_2)(c'_1 - I_1 s'_1)$$

---

## 7.3.4: Can you picture 3D rotation composition as a spherical triangle of half-angle arcs, and would that make rotation "intuition" finally concrete?

This is the "oh wow" geometric picture: rotor multiplication is spherical trigonometry in disguise. The scalar part of the product obeys the spherical law of cosines, so you can think of each rotor as a half-angle arc on a sphere; composing rotations is literally "slide the arcs so they connect, then the third side of the spherical triangle is the result." If you've ever felt rotation composition was opaque (matrices or quaternions often are), this picture makes it visual and mechanical: add arcs, complete the triangle, done.

**Key equation:**
$$\cos(\phi_t/2)=\cos(\phi_1/2)\cos(\phi_2/2)+\sin(\phi_1/2)\sin(\phi_2/2)\cos(\phi_\perp)$$

---

## 7.3.5: If unit quaternions are merely rotors ripped out of geometric algebra, what limitations of quaternions disappear the moment you stay in GA?

Unit quaternions stop being mystical the moment you see them as rotors living inside real 3D geometric algebra. The "imaginary" $i,j,k$ basis is just a bivector basis (coordinate-plane 2-blades), and quaternion multiplication is just the geometric product of those rotors. The killer advantage of staying in GA is scope: quaternions mainly rotate vectors in 3D, while rotors/versors rotate *any grade* and generalize cleanly beyond 3D without inventing new algebraic gadgets.

**Key equation:**
$$q_0+\vec{q}\ \leftrightarrow\ R = q_0 - q I_3$$

---

## 7.4: Why is the exponential form $e^{-B/2}$ the fastest route from "plane + angle" to an actual rotor you can compute with?

The exponential form is the shortest route from "I know the plane and angle" to "I have an operator I can apply." Instead of constructing a rotor as a product of vectors (and then worrying about normalization and interpretation), you directly write the rotor as $e^{-I\phi/2}$, where the bivector $I$ already *is* the oriented rotation plane and $\phi$ is the angle. That's why this parameterization becomes the workhorse later: it's a clean map from geometric intent to executable transformation.

**Key equation:**
$$R_I = e^{-I\phi/2}$$

---

## 7.4.1: What becomes simpler (or suddenly obvious) when you treat a rotor as an exponential of a 2-blade instead of juggling sine/cosine pieces?

Once you internalize that the power series of $e^{I\psi}$ splits into even powers (scalars) and odd powers (bivectors), rotor algebra becomes less like "expand trig identities" and more like "move exponents around." That pays off fast: commutation/anticommutation with the rotation plane tells you how things transform, and many derivations collapse to a couple lines. In practice, exponentials make it easier to reason about incremental rotations, composition when things commute, and smooth interpolation.

**Key equation:**
$$e^{I\psi}=\cos\psi + I\sin\psi$$

---

## 7.4.2: How can one exponential mechanism generate trig rotations, hyperbolic "boosts/scalings," and even the weirdly simple null case—just from the sign of $A^2$?

This is the punchline: the *same* exponential mechanism produces circular rotations, hyperbolic "boost-like" transformations, and the null case that collapses to $1+A$. The only thing that changes is the sign of $A^2$, i.e., what your metric says about the bivector's square. That's why GA scales so well into non-Euclidean models: you don't learn a new transformation language—your exponentials just switch from trig to hyperbolic (or truncate) when the geometry demands it.

**Key equation:**
$$\exp(A)=\begin{cases}\cos\alpha + A\sin\alpha,&A^2=-\alpha^2\\1+A,&A^2=0\\\cosh\alpha + A\sinh\alpha,&A^2=\alpha^2\end{cases}$$

---

## 7.4.3: Why can't every rotor in every space be written as $e^{-B/2}$, and exactly which spaces let you get away with it (and which don't)?

The "fine print" matters because exponentials are seductive: you might assume every rotor is $e^{-B/2}$ for some bivector $B$, but that fails in general spaces. The good news (and the reason you care) is that in the spaces you actually use most—Euclidean spaces and Minkowski spaces—every orthogonal transformation connected to the identity has exactly this bivector-generator form. So in those settings, "write the bivector, exponentiate it" is not a trick; it's the general solution.

**Key equation:**
$$L[x]=e^{-B/2}\,x\,e^{B/2}$$

---

## 7.4.4: If you want smooth interpolation and "Nth roots" of rotations, what does $\log(R)$ buy you—and why is picking the right log a subtle problem?

If you want interpolation that doesn't look like garbage, you need a way to "take smaller steps" along the same motion, and the clean way is: take the bivector log, divide, exponentiate. That immediately gives you meaningful $N$-step decompositions of a rotation or rigid motion. The catch—and why this section exists—is that $\log$ is multivalued (rotors wrap with 4π periodicity), and for general bivector exponentials you often need factorization into commuting pieces before you can extract a closed form, so the "simple idea" hides real geometric choices.

**Key equation:**
$$R^{1/N}=\exp(\log(R)/N)$$

---

## 7.5: What changes when you stop treating subspaces as passive things and start treating them as operators that act on other subspaces?

Here's the mindset shift: a blade isn't just a thing that *represents* a subspace—it can *act* as an operator on other objects. The reflection formula shows that a subspace $A$ can be used as a reflector via a sandwich, producing the correct transformed object $X'$ (including the sign behavior that depends on grades). Once you accept "subspaces are operators," a lot of geometry becomes operator algebra: mirrors reflect planes, planes reflect rotors, and you stop switching mental models between "object math" and "transform math."

**Key equation:**
$$X' = (-1)^{x(a+1)}A X A^{-1}$$

---

## 7.5.1: Why do reflection formulas explode into sign bookkeeping when you care about orientation and dual vs direct representation—and what breaks if you ignore it?

If you're sloppy about whether a blade is a *direct* subspace (like a plane bivector) or a *dual* one (like its normal), you will get sign errors that quietly flip orientations and break downstream logic. The chapter makes this explicit: the reflection signs depend on grade, and dualizing changes grades, so the correct sandwich has different sign rules depending on representation (summarized in their Table 7.1). This is the difference between "it kind of looks right" and "the reflected plane's normal is actually consistent with the reflected plane."

**Key equation:**
$$X' = (-1)^{x d}D X D^{-1}\quad(D=A^*)$$

---

## 7.5.2: How does projection become a "sandwich" too once you swap geometric products for contractions, and what does that tell you about projection as an operation?

Projection looks like a different beast in linear algebra, but in GA it snaps into the same sandwiching pattern—just swap the geometric product for contractions. That matters because it tells you projections are not ad hoc formulas; they're operators built from the same subspace objects you already have, and they carry clean algebraic properties: reflectors are involutions (do twice = identity), projectors are idempotent (do twice = once). When your "reflect," "rotate," and "project" all look like the same kind of expression, you can write far more uniform code and theory.

**Key equation:**
$$X' = (X\,\lrcorner\, A)\,\lrcorner\, A^{-1}$$

---

## 7.5.3: In nested transformations (like joint hierarchies), why does the correct update use a sandwich $R_2R_1\tilde{R}_2$ instead of plain multiplication $R_2R_1$?

This is the difference between "apply a rotation" and "rotate the *rotation itself*." In hierarchical systems (robot arms, camera rigs, skeletal animation), you constantly need the parent transform to *change the child's rotation axis/plane*, not just apply after it. The correct update is a sandwich: $R_2$ acts on the bivector plane inside $R_1$, producing a new rotor $R'_1$ whose plane is rotated. If you mistakenly use plain multiplication $R_2R_1$, you get concatenation, not nesting—and your kinematic chain will behave wrong.

**Key equation:**
$$R'_1 = R_2\,R_1\,\tilde{R}_2$$

---

## 7.6: If versors unify rotations, reflections, and "anti-rotations" as $\pm VXV^{-1}$, how much of your transformation toolkit collapses into one pattern?

The versor concept is the unification step: once you accept that "a product of vectors" is an operator, then reflections (odd products), rotations (even products), and rotation-plus-reflection ("antirotations") are all the same kind of sandwiching transform. Instead of memorizing separate transformation families, you work with one template $VXV^{-1}$ plus a parity-dependent sign rule. That's why versors are worth learning: they compress the whole orthogonal-transformation toolbox into one algebraic idiom.

**Key equation:**
$$V[X] = (-1)^{xv}V X V^{-1}$$

---

## 7.6.1: What's the payoff of compressing a whole chain of reflections into a single versor $V$ so the full transform is one sandwich and one inverse?

A chain of reflections is conceptually simple but computationally annoying if you treat each reflection as a separate step. Versors let you multiply the reflecting vectors once to form $V$, and then the whole chain becomes one sandwich. This is exactly how rotors already worked (even versors); versors just admit the general case and make it explicit that the geometric product is "operator multiplication" in disguise.

**Key equation:**
$$V = v_k\cdots v_2 v_1,\quad x' = V x V^{-1}$$

---

## 7.6.2: Why does "even vs odd number of vector factors" instantly tell you whether you preserve or flip handedness—without computing anything complicated?

You don't need to compute anything fancy to know whether a transform flips handedness: just look at parity. An even versor (even number of vector factors) has $\det=+1$ and behaves like a proper rotation; an odd versor has $\det=-1$ and includes a reflection, turning right-handed frames into left-handed ones. That's an immediate, structural classifier built into the representation—something matrices don't give you "for free."

**Key equation:**
$$\det(V)=(-1)^v$$

---

## 7.6.3: If versor sandwiches preserve inner products, why does that force them to preserve wedge products, contractions, and whole geometric constructions automatically?

This one identity explains why versors are so powerful: preserving inner products means the transformation is orthogonal, so it preserves lengths and angles. Because versor maps also extend as outermorphisms, wedge products are preserved too, which means oriented subspaces, intersections/unions built from them, and all the standard derived products keep their meaning after transformation. In other words: once you use versors, your *entire geometric construction pipeline* is stable under transformation, instead of needing custom "transform this object type" rules.

**Key equation:**
$$V[x]\cdot V[y]=x\cdot y$$

---

## 7.6.4: With blades, versors, rotors, and spinors all on the table, how do you tell what you actually have—and why does that choice matter for what you can do?

The taxonomy matters because people mix terms and then get confused: blades are outer products (subspaces), versors are geometric products (operators), rotors are even unit versors with $R^{-1}=\tilde{R}$, and spinors are defined via "sandwich a vector and get a vector" in the Clifford-group language. This expansion shows why bivector exponentials behave like "spinor/rotor-like" objects: repeated contraction of a vector with a bivector stays a vector, so the sandwich stays in the vector space. The punchline is blunt: rotors are the geometrically grounded operators you actually want, and "spinor" is mostly a broader physics/maths umbrella that almost always collapses back to rotors in the spaces you care about.

**Key equation:**
$$e^{-B/2} x e^{B/2}=x+(x\,\lrcorner\,B)+\frac{1}{2!}((x\,\lrcorner\,B)\,\lrcorner\,B)+\cdots$$

---

## 7.7: Why should you care about the *product* structure of geometric algebra—what does it give you that a stack of separate vector identities can't?

This is the chapter's structural claim: GA isn't a pile of unrelated tricks; it's a product system where one product (the geometric product) underlies operators, subspaces, and how they interact. Versor transformations preserve the geometric product, so anything you built multiplicatively stays consistent after transformation. That's why GA can replace a mess of separate linear-algebra constructions: you build geometry with products, then you transform with products, and the algebra guarantees the structure survives.

**Key equation:**
$$V[A]\,V[B]=V[AB]$$

---

## 7.7.1: If you had to reduce GA to a small "menu" of meaningful products, which ones cover most geometry and transformations without changing formalisms?

If you want the "minimal kit" of GA, it's basically this: the geometric product (which splits into metric information via $a\cdot b$ and subspace-spanning via $a\wedge b$), contractions (for mixed-grade metric interactions and projections), and versor sandwiching (for orthogonal transformations). Everything else in the chapter is a repackaging: reflection is a geometric-product sandwich, projection is a contraction sandwich, and rotations/rotors are even products of reflections. Once you see the menu this way, GA stops being "new math" and starts being "a smaller number of rules that cover more cases."

**Key equation:**
$$ab = a\cdot b + a\wedge b$$

---

## 7.7.2: Why is "you can add arbitrary multivectors" often geometrically meaningless, and how does a stricter GA viewpoint keep every constructed element interpretable?

Here's the uncomfortable truth: if you allow arbitrary sums of arbitrary grades as "new geometric entities," you quickly end up with symbols that don't correspond to anything you can name or reason about geometrically. The authors' stance is deliberately strict: build things multiplicatively so every constructed element is either a subspace (blade) or an operator (versor), with clear meaning; treat general multivector addition as a computational convenience for expansion, not a geometry-construction rule. The one big exception they admit is bivector addition under exponentiation, because exponentials of bivectors are still geometrically meaningful operators—but even there, the rule is "add to exponentiate," not "add to invent random objects."

**Key equation:**
$$\text{constructive addition allowed only for grades }k\in\{0,1,n-1,n\}$$

---

## 7.7.3: If you need speed, when do rotors beat matrices, when do they lose, and what's the smartest representation to use for the object you're transforming?

Efficiency is not magic: rotors win hard at *composition* because multiplying two rotors is fundamentally cheaper than multiplying two $n\times n$ matrices (roughly $2^n$ work vs $n^3$, and in practical dimensions 3–5 that's a big deal). But rotors can lose when you apply them naively to vectors, because a two-sided product looks expensive unless you exploit grade structure or convert to a matrix. The honest message is: GA gives you cleaner structure and often competitive performance, but you still need to choose representations intelligently (rotor for chaining, matrix or optimized code paths for bulk vector transforms, outermorphism matrices for k-blades, etc.).

**Key equation:**
$$\text{rotor storage}\sim 2^{n-1}\quad\text{vs}\quad\text{matrix storage}=n^2$$

---

## 7.8: If you want to go deeper, which references will actually clarify the rotor/spinor story instead of burying you in formalism?

If you want to go deeper without drowning, follow the references that actually connect rotors/spinors to geometry instead of treating them as abstract algebra objects. The chapter points you to core GA sources for rotors/spinors (and Lie-group connections) and then to more formal Clifford/spinor texts for the "what's always true, what has exceptions" fine print—useful when you're building implementations and don't want to rely on patterns that fail in some signatures. The key is to keep your target fixed: understand transformations as bivector-generated exponentials acting by sandwiches, because that's the backbone everything else hangs on.

**Key equation:**
$$L[x]=e^{-B/2} x e^{B/2}$$

---

## 7.9: How do these exercises stress-test whether you can *use* versors and rotors rather than just read about them?

The exercises are where you find out whether you actually *own* the material or just nodded along. Rotors and versors look simple on paper, but fluency means you can build them, normalize them, compose them, and apply them to non-vector objects without "going back to matrices" every time. The drill/structural split is deliberate: one part builds mechanical skill with the sandwich formulas, the other forces you to confront the subtle stuff (factorization, orientation signs, duality, degeneracies) that will otherwise ambush you later.

**Key equation:**
$$X' = R X \tilde{R}$$

---

## 7.9.1: Can you do the concrete rotor/reflection computations fluently enough that the chapter stops being theory and becomes a tool?

The drills are basically muscle memory for rotor arithmetic: compute a rotor, apply it, compose two, extract axis/angle, reflect blades, etc. Doing that a handful of times is what makes the abstraction feel concrete—because you start seeing $RXR^{-1}$ as "the rotation operator acting on whatever $X$ is," not as a special formula that only works for vectors. If you can do these quickly, you've crossed the line from reading GA to using GA.

**Key equation:**
$$R[X]\equiv RXR^{-1}$$

---

## 7.9.2: Which subtle gotchas (factorization, sign conventions, duality, degeneracy) will these structural problems inoculate you against before they wreck your intuition or code?

The structural problems are there to prevent classic mistakes that otherwise look like "random sign bugs" or "why did my normal flip?" months later. The parity-dependent sign $(-1)^{xv}$ is exactly the kind of detail you can ignore until it burns you, and duality makes it worse because grades change under dual representation. These exercises force you to track what's actually being represented (direct vs dual), what grade it has, and what happens under transformation—so your intuition becomes robust enough to survive real implementations.

**Key equation:**
$$V[X]=(-1)^{xv}VXV^{-1}$$

---

## 7.10: What does the chapter's theory look like when it hits real code—and where does it genuinely simplify implementation?

This section's job is to prove the chapter isn't just theory: the same sandwiching operator you've been manipulating symbolically drops straight into code, often as a literal one-liner. The examples also show the practical boundary: yes, you sometimes convert rotors to matrices (e.g., to interface with OpenGL), and yes, you care about numerical stability and speed. The point is that GA gives you a single conceptual pipeline (build operators geometrically → apply by sandwiches → optimize where needed) instead of a zoo of special-case math types.

**Key equation:**
$$X' = V X V^{-1}$$

---

## 7.10.1: What changes in your coding life when reflection is literally one line—and the same pattern works for bivectors too?

The reflection demo is almost insulting in how simple it is: implement reflection as $a x a^{-1}$ and you're done. No coordinate decomposition, no explicit "mirror matrix," no special casing—and the exact same pattern extends beyond vectors if you let $x$ be a bivector (plane) or other multivector element. That's the real win: the math translates into code with essentially zero translation overhead.

**Key equation:**
$$\texttt{reflect}(x)=a x a^{-1}$$

---

## 7.10.2: Can you implement a rotation without trig or matrices by doing just two reflections and still get the right behavior interactively?

Two reflections equal one rotation isn't just a theorem you memorize; it's an implementation strategy. You can literally code a rotation as "reflect twice," then observe that algebra collapses it to a single rotor sandwich anyway. This matters because it ties your geometric intuition (a rotation is a double-mirror effect in a plane) directly to a robust computational primitive, and it generalizes without inventing cross products or axis-angle edge cases.

**Key equation:**
$$x' = b(a x a^{-1})b^{-1} = R x R^{-1}$$

---

## 7.10.3: What are the practical numerical landmines in rotor↔matrix conversion, especially near 180° rotations?

Matrix↔rotor conversion is where "clean theory" meets numerical ugliness. The compact "rotate $a$ to $b$" rotor formula is great—until $a\cdot b\approx-1$, i.e., you're near a 180° turn, where the denominator collapses and the rotation plane becomes geometrically ambiguous. The section is blunt about it: naïve conversion procedures become unstable right where you most need robustness, so you either pick safer basis vectors, inject a fallback plane, or use a more stable conversion routine.

**Key equation:**
$$R=\dfrac{1+ba}{\sqrt{2(1+b\cdot a)}}$$

---

## 7.10.4: How much speed do you gain by deriving the closed-form conversion formulas once, and why does that matter in tight inner loops?

Deriving the closed-form rotor→matrix mapping once saves you from doing three expensive sandwich products every time you need a rotation matrix. You symbolically expand $R e_i\tilde{R}$ on the basis vectors, read off the coefficients as matrix columns, and you get the same classic quaternion-style conversion—but now you know exactly where it came from and how it fits the GA picture. In real code, that's not academic: it's the kind of algebraic precomputation that turns "nice but slow" into "nice and fast."

**Key equation:**
If $R=w+x\,e_{23}+y\,e_{31}+z\,e_{12}$, then $R e_1\tilde{R}=(1-2y^2-2z^2)e_1+2(-wz+xy)e_2+2(wy+xz)e_3$

---

## 7.10.5: How do Julia fractals work when you ditch complex numbers and compute them with real vectors—so the same algorithm lifts to 3D and beyond?

The Julia fractal example is there to show subsumption isn't a slogan: once complex numbers are "vector ratios," the same fractal iteration becomes a pure real-vector computation. For $p=2$, you replace complex multiplication with a geometric product and get $x_{k+1}=x_k e x_k + c$, which is still a vector—so the algorithm doesn't care if you're in 2D, 3D, or $n$D. That's the hook: you get the classic 2D picture *and* a straight path to higher-dimensional "quaternionic" (really GA) fractals without changing the core idea.

**Key equation:**
$$x_{k+1}=x_k e x_k + c \quad\text{(from } X_{k+1}=X_k^2+C\text{)}$$

---

## 7.10.6: Why is "update orientation by $\exp(\text{motion}\wedge\text{something})$" such a clean, stable way to turn mouse motion into 3D camera rotation?

This is what "rotors are practical" looks like: treat mouse motion as a tiny bivector (a plane element) and exponentiate it to get an incremental rotor update. The code literally updates the model rotor by multiplying by $\exp(\text{small bivector})$, either in the screen plane $(m\wedge p)$ or out of plane $(m\wedge e_3)$. The payoff is stability and geometric correctness: you're integrating orientation changes in the natural Lie-group way (multiply by small rotors), instead of fighting gimbal lock or accumulating matrix drift.

**Key equation:**
$$R \leftarrow \exp(\varepsilon(m\wedge p))R$$
