# Why Geometric Algebra Code Gets Slow

Geometric algebra feels like it should be slow for three reasons that are obvious the moment you see it implemented: a general multivector in an $n$-dimensional algebra has $2^n$ coordinates; practical geometric models (notably the conformal model of Euclidean 3D) live in higher-dimensional algebras with correspondingly larger bases; and there's a long menu of products and operations you can apply to "anything." If you implement that universality directly, you usually get a system that is beautiful and unusable on high-volume geometry workloads.

The chapter's key claim is that this "inefficiency" is less about the algebra and more about **implementation overhead**. Early GA libraries tended to be general-purpose first, performance second, and they accidentally chose the worst tradeoffs for modern CPUs: lots of indirection, branching, and runtime decisions wrapped around tiny arithmetic kernels.

The diagnosis collapses into three recurring bottlenecks:

- **Multivectors are too general.** Treating every variable as a full multivector forces you to store and touch far more coordinates than most geometric entities actually use. Most "geometrically sensible" objects only occupy limited grades and specific basis blades, so making them all "multivector" is mathematically uniform but computationally wasteful.
- **The metric can't be a runtime variable if you care about speed.** The metric affects the most basic products, so implementing the metric as a table lookup in the inner loop turns every multiply-add into "multiply-add plus memory access and indexing overhead." The chapter's stance is blunt: for efficiency, you generate a **separate implementation per metric** you actually use.
- **Operations are "small," so overhead dominates.** Many GA products are not complicated when written out in coordinates; that's exactly why generic implementation overhead (loops, branches, virtual dispatch, table lookups) can become a large fraction of total time. The chapter pushes you toward fusing expressions into single optimized calculations rather than executing them as chains of generic calls.

From these bottlenecks, the chapter derives a practical performance target that looks less like "do fewer flops" and more like "respect the CPU": store variables compactly; avoid touching zeros; minimize unpredictable memory access; avoid conditional branches (branch mispredicts are expensive); unroll loops where possible; and optimize whole expressions rather than treating each operation as an isolated unit.

A deliberately naive reference implementation (built around generic multivectors, runtime metric lookup, and loop-based coordinate processing) fails almost all of these demands: it pays bookkeeping overhead to compress coordinates, it consults tables for the metric, it branches and loops in hot paths, it leaves loops rolled, and it executes composed expressions as one generic call after another. The chapter's conclusion is that to get "linear-algebra-like" performance, you must intentionally give up some of the universal structure at runtimeâ€”by specializing representation, hard-coding the metric, and compiling high-level expressions into low-level straight-line code.
