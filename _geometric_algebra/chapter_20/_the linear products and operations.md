# The Linear Products and Operations

This chapter is really about one move: **turn "multiply by $A$" into "apply a linear operator"**—and then choose how you want to represent that operator in code. Once you do that, *every* linear product (geometric, outer, contractions, scalar, commutator) and the common unary linear operations (reversion, grade involution, grade extraction, …) become variations on the same theme: linearity + distributivity, implemented either densely (matrices) or sparsely (lists).

The story starts by fixing an ordering of basis blades and using it to encode any multivector as a length-$2^n$ coordinate object; that encoding is the bridge that lets you import "ordinary" linear algebra into geometric algebra without changing the underlying math. The moment you have that bridge, basic linear structure is no longer mysterious—it becomes direct array arithmetic, and the "fancy" unary operators are just predictable, basis-dependent sign flips and selections. That's the dense, comfortable world opened up by [[basis encoding of multivectors]] and made operational by [[unary linear operators as diagonal matrices]].

Then the chapter hits the only genuinely nontrivial step: **products**. A product like $AB$ is linear in $B$, so with $A$ frozen it is literally a linear map on the coordinate vector of $B$. But unlike the unary ops, that map depends on the coefficients of $A$. So "the geometric product" does not correspond to one fixed matrix; instead, **every left factor $A$ induces its own matrix**. The way you build that matrix is by using the algebra's multiplication table on basis blades—captured compactly as "structure coefficients"—so the implementation is "just" bookkeeping over those coefficients. That whole construction is the heart of [[geometric product matrix from structure coefficients]].

Once you can build the geometric-product operator, the rest of the product zoo is revealed as grade-filtering: wedge and contractions are not new multiplications so much as **grade-selected views** of the geometric product. In matrix form, that shows up as the same skeleton with systematic zeros and recognizable structure (triangularity, identity-column behavior, etc.), and it motivates bootstrapping via symbolic product matrices that you instantiate numerically at runtime. That perspective is unpacked in [[outer product and contraction matrices as selections]].

Finally, the chapter refuses to pretend dense matrices are the whole story. In geometric algebra practice, most objects are sparse: blades live in one grade, versors often live in even/odd subspaces, and "most coordinates are zero most of the time." Dense $2^n \times 2^n$ operators don't exploit that, so you switch representations: store multivectors as lists of weighted basis blades and push distributivity down to the blade level. It's conceptually clean and automatically sparse—but the naive double loops are expensive, which is why the chapter flags optimization as a later concern. That alternative implementation worldview is [[basis-blade list implementation and sparsity]].
