The vector space model is geometric algebra used in its most stripped-down, honest form: everything is a *direction at the origin*. That sounds limiting until you realize that a huge fraction of geometry is really about *attitudes* (which subspace?), *weights* (how much?), and *orientation* (which way?), not about where things sit. This chapter leans into that purity, treating the geometric algebra of $\mathbb{R}^n$ as an "algebra of directions," and then stress-testing how far that gets you before you truly need translations.

Once you buy that premise, the chapter's first move is to show that "angle-chasing" becomes "algebra-chasing." A planar triangle is no longer three points and some fragile sign conventions; it's just one vector equation plus a few carefully chosen geometric ratios, and classical trigonometry drops out when you split products into scalar and bivector parts in [[planar triangles with rotors]]. In 3D, the idea scales: angles are best treated as *oriented* objects (plane + amount), and geometric ratios between vectors and bivectors already *are* those oriented angles, which lets spherical triangle identities fall out of rotor products in [[angles in 3d and spherical triangles]].

Then the chapter pivots to a different kind of "angular structure": local symmetry. In crystals, what matters at a point is which reflections and rotations can be composed without generating an infinite mess. Geometric algebra makes the operators themselves tangible—products of reflection normals—and that turns "group generation" into straightforward multiplication, as shown in [[point groups via reflections and rotors]].

After the conceptual tour, the chapter gets pragmatic: if rotors are your rotation objects, you need reliable ways to *construct* them from geometry, *extract* their generating bivectors (logarithms), and *move between* orientations smoothly (interpolation). That toolbox—plane+angle to rotor, vector-to-vector rotor, frame-to-frame rotor, rotor log, and slerp-style interpolation—lives in [[computing 3d rotors]].

Finally, it confronts reality: in vision/robotics you don't "get" rotations; you estimate them from noisy directional data. The external camera calibration example shows how the vector space model can mix directions (image rays) with locations (camera centers, world points) in a single optimization loop—powerful, but also a reminder that positions are being handled as a "convenient abuse" that stops being structural the moment you want uniform translation of general objects. That entire estimation story and the built-in warning label are in [[estimation and camera calibration in the vector space model]].
