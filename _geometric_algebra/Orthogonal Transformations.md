# Chapter 7 — Orthogonal Transformations as Versors

Chapter 2 was about *spanning*: you can talk about subspaces without ever mentioning length.  
Chapter 7 is where the metric comes crashing back in—because **transformations need inverses**, and inverses need a notion of “non‑zero squared length.”

This chapter’s core claim is blunt:

> **Orthogonal transformations are products of reflections.**  
> In geometric algebra, that means they are **products of vectors**.  
> Not matrices. Not coordinates. Just geometry multiplied together.

**What we’ll build:** a practical operator calculus where  
- reflections are *sandwiches*,
- rotations are *double reflections*,
- compositions are *just products*,
- and the general object behind all of it is the **versor**.

---

## 1. Why This Chapter Exists: Matrices Don’t “Know” Geometry

Linear algebra represents an orthogonal transformation as an orthogonal matrix $M$ with $M^\top M = I$. That works, but it’s not geometric:

- The matrix depends on a coordinate basis.
- The “meaning” (axis, plane, angle, handedness) is not visible in the object.
- Rotating anything besides vectors (planes, volumes, operators…) requires extra machinery.

Geometric algebra flips that around: it starts from a geometric primitive (reflection), and *builds everything else from it*.

---

## 2. The Primitive Move: Reflection as a Sandwich

### 2.1 Line reflection (a “flip” in a direction)

If a line through the origin is characterized by a (non-null) vector $a$, the reflection of a vector $x$ in that line is

$$
x \mapsto a x a^{-1}.
$$

This is the “sandwiching” pattern: **operator – object – inverse**.  
Because reflection is linear, it extends to blades and multivectors by outermorphism:

$$
X \mapsto a X a^{-1}.
$$

Key point: **the scale of $a$ does not matter**—it cancels between $a$ and $a^{-1}$. Only the attitude matters.

### 2.2 Hyperplane reflection (the mirror you actually mean in 3D)

A reflection “in a mirror” is a hyperplane reflection. If $a$ is the *normal* vector of the reflecting hyperplane (dual representation), then:

$$
x \mapsto - a x a^{-1}.
$$

This is the one whose determinant is $-1$ in *any* dimension, so it behaves like a true reflection.

### 2.3 The annoying but important nuance

A “line reflection” is not always a true reflection in the determinant sense. In $n$ dimensions its determinant is $(-1)^{n+1}$. That means:

- In 3D: a line reflection has determinant $+1$, so it behaves like a **rotation** (specifically, a 180° turn around that line).

So when you say “reflection,” you almost always mean **hyperplane reflection**, not line reflection.

#### Quick comparison

| Mirror described by | Geometric object | Action on $x$ | Determinant |
|---|---|---:|---:|
| vector $a$ as **line direction** | line through origin | $x \mapsto a x a^{-1}$ | $(-1)^{n+1}$ |
| vector $a$ as **plane normal** | hyperplane | $x \mapsto -a x a^{-1}$ | $-1$ |

---

## 3. Two Reflections = One Rotation (Rotors)

This is the workhorse theorem: **any rotation is an even number of reflections.**

### 3.1 The construction

Reflect in $a$, then reflect in $b$:

$$
x \mapsto b(a x a^{-1})b^{-1}
      = (b/a)\, x \, (b/a)^{-1}.
$$

So a double reflection is generated by the element

$$
R = b/a = b a^{-1}.
$$

That $R$ is called a **rotor** when normalized.

### 3.2 Normalize once, then life is easy

If $a$ and $b$ are unit vectors, then

- $R = b a$
- $R^{-1} = a b = \tilde{R}$ (reverse)

and the rotation of *any* multivector $X$ is

$$
X \mapsto R X \tilde{R}.
$$

That one formula rotates vectors, bivectors (planes), trivectors (volumes), you name it. No separate “how to rotate a plane” routine.

### 3.3 Rotor = “scalar + bivector” (and the half-angle)

For unit vectors $a,b$, their geometric product splits into inner and outer parts:

$$
R = b a = b\cdot a + b\wedge a.
$$

If the rotation angle is $\phi$ and the rotation plane is the unit 2‑blade $I$, then

$$
R = \cos(\phi/2) - I \sin(\phi/2).
$$

The rotor uses a **half-angle**. That is not a cosmetic choice. It’s the whole reason rotors behave like spinors.

### 3.4 What the sandwich actually does

Decompose $x$ into components relative to the rotation plane:

- $x_{\parallel}$: projection into the plane
- $x_{\perp}$: rejection perpendicular to the plane

Then $R x \tilde{R}$ does exactly what you want:

- $x_{\perp}$ stays unchanged
- $x_{\parallel}$ rotates in the plane by $\phi$

This is not magic. It’s algebra enforcing the geometry.

---

## 4. The “Same Rotation” Problem: Why $R$ and $-R$ Both Work

Because the rotor action is $R x \tilde{R}$, the sign cancels:

$$
(-R) x (-\tilde{R}) = R x \tilde{R}.
$$

So $R$ and $-R$ rotate an *isolated* vector the same way.

But that does **not** mean the representation is redundant. The chapter makes this point using the famous “plate trick” / “belt trick” idea:

- A $2\pi$ rotation returns a free object to the same orientation.
- A $2\pi$ rotation does **not** return a *connected* object (with a path / wiring) to the same state.
- A $4\pi$ rotation does.

So rotors encode something extra: a **sense of rotation** (a path in rotation space), not just the final pose.

**Bottom line:** rotors form a *double covering* of the usual rotation group.  
That sign carries geometric meaning about *how* you got there, not just where you ended up.

---

## 5. Composition of Rotations: Multiplication, Not Matrix Pain

If you rotate by $R_1$ and then by $R_2$, the net rotor is

$$
R = R_2 R_1.
$$

That’s it. Associativity of the geometric product makes the proof trivial.

### 5.1 2D: Rotors behave like complex numbers (but cleaner)

In a single plane with pseudoscalar $I$, planar rotors commute and angle-add:

$$
R_{I,\phi_2} R_{I,\phi_1} = R_{I,\phi_1+\phi_2}.
$$

Also, in a plane you can simplify the sandwich to a one-sided product:

$$
x \mapsto x(\cos\phi + I\sin\phi),
$$

because vectors in the plane anticommute with $I$.

**Complex numbers show up here** as a *representation trick*: a complex number is basically the ratio $x/e$ relative to a chosen “real axis” vector $e$. That choice breaks symmetry. GA doesn’t need that hack: it keeps vectors and operators distinct in the same algebra.

### 5.2 3D: Rotors subsume unit quaternions

In 3D, any rotor is still “scalar + bivector” (because there is no grade 4). When you multiply rotors, you get exactly the quaternion multiplication structure—dot products and cross products appear.

But GA’s message is sharp:

- quaternions are rotors in disguise,
- quaternion multiplication is rotor multiplication,
- and rotors are **more general** (they rotate blades of any grade, and they work in $n$ dimensions).

### 5.3 Visualizing 3D composition: the “rotation sphere”

The chapter gives a real geometric visualization of quaternion/rotor multiplication:

- represent a rotor by a **half-angle arc** on a sphere (great circle determined by the rotation plane),
- compose rotations by “sliding” arcs and completing a spherical triangle (spherical cosine law).

This turns “quaternion multiplication is mysterious” into “it’s spherical geometry.”

---

## 6. Exponentials: Rotors as $e^{-B/2}$

### 6.1 Pure rotors are exponentials of 2‑blades

For a Euclidean unit 2‑blade $I$ with $I^2 = -1$,

$$
R = \cos(\phi/2) - I\sin(\phi/2) = e^{-I\phi/2}.
$$

This drops straight out of the power series: even powers become cosine terms, odd powers become sine terms.

### 6.2 Trig vs hyperbolic vs “nilpotent” (this matters later)

The exponential depends on the square of the blade:

- $A^2 < 0$  → trigonometric functions (rotations)
- $A^2 = 0$  → the series truncates (translations later)
- $A^2 > 0$  → hyperbolic functions (scalings / boosts later)

So the same exponential machinery will later cover Euclidean rotations, translations, and scalings—once we move to the right model space.

### 6.3 Exponentials of bivectors: the fine print

In 2D and 3D, “bivector” and “2‑blade” coincide. In higher dimensions they don’t.

The chapter’s (slightly painful) reality check:

- Not every rotor in every metric is $e^{\text{(bivector)}}$.
- But in the Euclidean and Minkowski spaces that matter most for applications, every orthogonal transformation *connected to the identity* can be written as

$$
x \mapsto e^{-B/2} x e^{B/2},
$$

for some bivector $B$.

Also: **do not** assume exponentials add:

$$
e^B e^A \neq e^{A+B}
$$

when $A$ and $B$ don’t commute.

### 6.4 Logarithms (why they’re hard)

To extract “angle + plane” from a rotor, you want a logarithm. In 3D this is manageable; in higher dimensions it can require factoring a bivector into commuting 2‑blades. That’s why the book postpones “general closed form” formulas.

---

## 7. Subspaces as Operators (and the Sign Hell You Can’t Ignore Forever)

### 7.1 Reflection in an arbitrary subspace

Let $A$ be a blade representing a reflecting subspace (direct representation). Then for vectors:

$$
x \mapsto x - 2\frac{x\wedge A}{A} = -A x A^{-1}.
$$

For a blade/multivector $X$, **orientation matters**, and each grade contributes a sign:

$$
X \mapsto (-1)^{x(a+1)} A X A^{-1},
$$

where $x=\mathrm{grade}(X)$ and $a=\mathrm{grade}(A)$.

### 7.2 Direct vs dual representation changes signs

If you represent the mirror dually as $D=A^\*$ (so $\mathrm{grade}(D)=n-a$), the reflection formula has a different sign pattern:

$$
X \mapsto (-1)^{x d} D X D^{-1}.
$$

This is why you can’t mindlessly “just dualize” everything and hope signs stay the same. Table 7.1 in the chapter is basically a sign accounting ledger.

**Pragmatic note:** if you don’t care about orientation bookkeeping, everything collapses to the simple-looking
$$
X \mapsto A X A^{-1}.
$$
But if you *do* care (normals staying normal, handedness tracking, duals interpreted in the original pseudoscalar), you have to do it properly.

### 7.3 Projection is also sandwiching (but with contractions)

Reflection uses geometric-product sandwiching and is an **involution** (do it twice, you get identity).

Projection uses contraction sandwiching:

$$
X \mapsto (X\!\;\lrcorner\! A)\!\;\lrcorner\! A^{-1},
$$

and is **idempotent** (do it twice, it’s the same as once).

Same high-level pattern, different algebraic engine.

### 7.4 Transformations are objects

You can rotate a rotor, reflect a rotor, etc. This matters in hierarchical modeling and robotics.

Key slogan:

> **Concatenated transformations use the geometric product.**  
> **Nested transformations use sandwiching.**

So “apply $R_2$ after $R_1$” is $R_2R_1$,  
but “rotate the operator $R_1$ by $R_2$” is $R_2 R_1 \tilde{R}_2$.

---

## 8. Versors: The General Orthogonal Transformation

### 8.1 Definition

A **$k$-versor** is a geometric product of $k$ invertible vectors:

$$
V = v_k v_{k-1}\cdots v_1.
$$

The corresponding transformation is a versor sandwich. After $k$ reflections, everything collapses into one operator.

### 8.2 Even vs odd versors

Parity is everything:

- **even versors** → rotations (det $=+1$)
- **odd versors** → reflections / “antirotations” (det $=-1$)

The chapter gives explicit even/odd sandwich formulas. The distinction is not cosmetic: it’s exactly the handedness behavior.

### 8.3 The big theorem: orthogonal transformations ⇔ versor products

Versor products preserve inner products:

$$
V[x]\cdot V[y] = x\cdot y.
$$

That is literally the definition of orthogonality. So versors are not “a nice subclass.” They are **exactly** orthogonal transformations.

### 8.4 Structure preservation (why this is a programmer’s dream)

A versor product preserves the geometric product, and therefore preserves every construction that is built from it (by grade selection):

$$
V[AB] = V[A]V[B],\qquad
V[A\wedge B] = V[A]\wedge V[B],\qquad
V[A\!\;\lrcorner\! B] = V[A]\!\;\lrcorner\! V[B],\ \ldots
$$

Even exponentials behave nicely:

$$
V[\exp(B)] = \exp(V[B]).
$$

This is why GA code can be insanely clean: you implement one versor action, and the algebra guarantees your derived geometry stays consistent.

### 8.5 Versors, blades, rotors, spinors (relationships)

- **Versor:** product of invertible vectors.
- **Rotor:** even versor with $R^{-1}=\tilde{R}$ and $R\tilde{R}=1$.
- **Invertible blade:** can be written as a geometric product of mutually orthogonal vectors, so it is a versor.
- **Spinor (physics literature):** closely related; all rotors are special spinors, and “almost all” special spinors are rotors (rare exceptions in dimensions $n \equiv 0 \pmod 4$).

---

## 9. The Product Structure of Geometric Algebra (and a Shot at “Just Clifford Algebra”)

By this point the book argues that most of geometry is:

- multiplication,
- (anti)commutation,
- grade selection.

It summarizes the main products:

- outer product (spanning),
- scalar product (angles/norms),
- contractions (mixing grades, projections),
- versor product (orthogonal operators),
- geometric product (the universal multiplication behind all of it),
- plus meet/join (geometric intersection/union for blades, nonlinear in degeneracy).

### GA vs Clifford (the author’s opinionated take)

Both live in the same multivector space and share the geometric product.

But:

- **Clifford algebra** treats arbitrary addition of multivectors as a fundamental “construction step.”
- **Geometric algebra (as used here)** treats multiplication as the meaningful construction step, and views general linear combinations mostly as coordinate bookkeeping (useful, but not inherently geometric).

Translation: if you can’t explain the geometric meaning of a sum you just wrote down, you may be doing Clifford algebra gymnastics, not geometry.

---

## 10. Efficiency and Implementation: Does This Actually Run Fast?

Yes—if you don’t implement it naively.

Key points from the chapter’s performance discussion:

- **Storage:** rotors (and rotors-as-quaternions in 3D) store rotations with far fewer parameters than matrices in low dimensions.
- **Composition:** composing orthogonal transforms is cheaper with rotors than matrices in practical dimensions.
- **Application to vectors:** multiplying a matrix by a vector is usually faster than a literal sandwich product, so in practice you may convert a rotor to a matrix when applying to many vectors.
- **Blades:** for transforming $k$-blades, **outermorphism matrices** can be very efficient.
- **Best case:** use code generation + grade knowledge + orthogonality structure. The book claims overhead can be kept to roughly 5–10% if you do it right.

The chapter then gives concrete code:
- reflecting vectors,
- showing two reflections = one rotation,
- matrix ↔ rotor conversion (OpenGL‑friendly),
- plus a fun “Julia fractal” demo computed using real vectors and the geometric product.

---

## 11. Summary (What You Should Actually Remember)

1. **Reflection is the primitive.** In GA it’s a sandwich product.
2. **Rotations are double reflections.** A rotor is an even product of unit vectors.
3. **The half-angle is not optional.** It gives rotors their spinor behavior ($4\pi$ periodicity, sense of rotation).
4. **Composition is multiplication.** $R = R_2R_1$.
5. **Complex numbers and quaternions are special cases.** Useful, but incomplete as geometric languages.
6. **Exponentials unify motion.** $e^{-B/2}$ is the native parameterization of “orthogonal motion” in the spaces we care about.
7. **Subspaces can act as operators.** Reflection and projection are both sandwiching—geometric vs contraction.
8. **Versors are the general orthogonal transformation.** And they preserve structure, so your geometry doesn’t fall apart.
9. **GA’s selling point is universality.** Same operator acts on vectors, planes, volumes, and even other operators.
10. **It’s implementable.** But you must exploit structure; brute-force multivector arithmetic is a trap.

---

## Appendix: Quick Reference (Formulas You’ll Reuse)

### Reflection

- Line (direction $a$):  
  $$
  x' = a x a^{-1}
  $$
- Hyperplane (normal $a$):  
  $$
  x' = -a x a^{-1}
  $$

### Rotation (rotor)

- Apply:
  $$
  X' = R X \tilde{R}
  $$
- From angle $\phi$ and unit plane $I$ (Euclidean):
  $$
  R = \cos(\phi/2) - I\sin(\phi/2) = e^{-I\phi/2}
  $$
- Compose:
  $$
  R_{\text{total}} = R_2 R_1
  $$

### Versors

- $k$-versor:
  $$
  V = v_k\cdots v_1
  $$
- Orthogonal action (conceptual form):
  $$
  X' = \pm V X V^{-1}
  $$
  (sign/parity bookkeeping depends on whether $V$ is even/odd and whether $X$ is treated grade-wise)

### Nested vs concatenated

- Concatenate operators: multiply them.
- Transform an operator: sandwich it.

That distinction saves you from *so many* modeling mistakes.
