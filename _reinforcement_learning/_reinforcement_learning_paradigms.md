# Reinforcement Learning Paradigms in Robotics

## On-Policy vs. Off-Policy Learning

**On-policy** methods require the agent to learn exclusively from data generated by its current policy. The policy parameters $\theta$ are updated via the policy gradient:

$$\nabla_\theta J(\theta) = \mathbb{E}_{s \sim d^{\pi_\theta}, a \sim \pi_\theta(\cdot|s)}[\nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)]$$

Where:
- $J(\theta)$ is the expected discounted return under $\pi_\theta$
- $Q^{\pi_\theta}(s,a)$ is the action-value (expected return after taking action $a$ in state $s$)
- Equivalently, one can use the advantage $A^{\pi_\theta}(s,a) = Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)$

The primary drawback is **sample inefficiency**: once the policy updates, all past experiences become obsolete and must be discarded.

**Off-policy** methods decouple the *target policy* (being improved) from the *behavior policy* (generating data). For example, Q-learning improves a greedy policy while data may come from an ε-greedy or random policy. This enables:

- **Sample efficiency**: reuse of large [[replay_buffer|replay buffers]] or offline datasets
- **Flexibility**: policy improvement without discarding old data
- **Practical deployment**: critical for robotics, where fresh on-policy data collection is expensive or unsafe

Off-policy learning updates Q-values toward a greedy Bellman target:

$$Q(s,a) = r + \gamma \max_{a'} Q(s',a')$$

DQN demonstrated that [[replay_buffer|experience replay]] and [[target_network|target networks]] stabilize this process. Extensions like **DDPG**, **TD3**, and **SAC** advanced it further—replay enables multiple updates per transition, and entropy regularization in SAC encourages exploration and robustness.

## Offline Learning

When live interaction is limited, expensive, or unsafe, **offline** (or batch) RL trains entirely from a fixed, pre-collected dataset. This introduces **distributional shift**: the agent may overestimate Q-values for out-of-distribution actions it has never observed.

Specialized algorithms address this:
- **Conservative Q-Learning (CQL)**: penalizes values of unseen actions
- **Implicit Q-Learning (IQL)**: reshapes the objective to avoid overestimation

These methods enable reliable improvement over logged behavior, provided the dataset is sufficiently diverse.

## Modern Hybrid Workflow

State-of-the-art robotics increasingly uses a two-stage **offline-to-online** approach:

1. **Offline Pre-training**: Initialize a policy on a large static dataset (e.g., human demonstrations) using conservative algorithms like CQL or IQL—providing a safe, well-generalized starting point.

2. **Online Fine-tuning**: Refine with efficient off-policy algorithms (SAC, TD3) to adapt to real-world dynamics and optimize beyond the original dataset's limitations.

This synthesis harnesses the safety and broad coverage of offline learning with the adaptive power of online learning, forming a cornerstone for complex manipulation and locomotion tasks.
