I assume you know, gradient of $f(x) = 0$ and Hessian $f''(x) \; or \; H < 0$  at minimum point.

- [youtube playlist](https://www.youtube.com/playlist?list=PL6EA0722B99332589)
- [[line_search_techniques]]
- [[global_convergence_theorem]]
- [[steepest_descent_method]]
- [[ellipse_into_circular_contour]]
- [[classical_newton_method]]
- [[modified_newton_method]]
- [[modified_newton_method_trust_region_]]
- [[quasi_newton_methods]]
	- [[rank_one_method]]
	- [[rank_two_methods]]
	- [[dual]]
	- [[broyden_family]]
- [[coordinate_descent]]
- [[conjugate_directions]]
- [[conjugate_vs_quasi_newton]]
- [[constrained_optimization_local_and_global_solutions_conceptual_algorithm]]
- [[feasible_and_descent_directions]]
- [[first_order_kkt_conditions]]
- [[interpretation_of_lagrange_multipliers]]
- [[existence_and_uniqueness_of_lagrange_multipliers]]
- [[second_order_kkt]]
- [[weak_and_strong_duality]]
- [[primal_dual_problem]]
- [[wolfe_dual]]
- [[linear_programming]]
- [[basic_and_non_basic_variable]]
- [[simplex]]
- [[two_phase_method]]
- [[interior_point]]
- [[affine_scaling]]
- [[karmarkars_method]]
- [[lagrange_methods]]
- [[active_set]]
- [[barrier_and_penalty_methods]]
- [[barrier_methods]]
- [[penalty_method]]
- [[augmented_lagrangian_method]]
- [[cutting_plane_methods]]