# Advanced Policy Gradients

## Policy Gradient is Just Policy Iteration

Recall the standard Policy Gradient update:

$$\nabla_\theta J(\theta) \approx \frac{1}{N}\sum_{i,t} \nabla_\theta \log \pi_\theta(a_{i,t} \mid s_{i,t}) \hat{A}_{i,t}^\pi$$

This looks suspiciously like Policy Iteration:

1. **Policy Evaluation:** Estimate $A^\pi(s,a)$ (the "critic" part).
2. **Policy Improvement:** Update $\pi$ to favor actions with high advantage.

### The Mathematical Link

We can express the exact difference in performance between two policies $\pi_\theta$ and $\pi_{\theta'}$ using the **Performance Difference Lemma**:

$$J(\theta') - J(\theta) = \mathbb{E}_{\tau \sim p_{\theta'}(\tau)} \left[ \sum_t \gamma^t A^{\pi_\theta}(s_t, a_t) \right]$$

If we maximize the term on the right, we maximize the improvement.

---

## The Distribution Mismatch Problem

We want to find $\theta'$ to maximize:

$$\text{maximize } \mathbb{E}_{\tau \sim p_{\theta'}(\tau)} \left[ \sum_t \gamma^t A^{\pi_\theta}(s_t, a_t) \right]$$

**The Problem:** The expectation is over trajectories generated by the new policy $\theta'$, but we only have samples from the old policy $\theta$.

We can use **Importance Sampling**:

$$\mathbb{E}_{s \sim p_{\theta'}} \left[ \dots \right] = \mathbb{E}_{s \sim p_{\theta}} \left[ \frac{p_{\theta'}(s)}{p_{\theta}(s)} \dots \right]$$

However, the ratio of state marginals $\frac{p_{\theta'}(s)}{p_{\theta}(s)}$ is incredibly difficult to estimate (it depends on the dynamics of the environment over time).

**The Hack:** Standard Policy Gradient simply ignores this state distribution mismatch. It assumes $p_{\theta'}(s) \approx p_{\theta}(s)$.

$$J(\theta') - J(\theta) \approx \sum_t \mathbb{E}_{s_t \sim p_\theta} \left[ \mathbb{E}_{a_t \sim \pi_\theta} \left[ \frac{\pi_{\theta'}(a_t|s_t)}{\pi_{\theta}(a_t|s_t)} \gamma^t A^{\pi_\theta}(s_t, a_t) \right] \right]$$

**Question:** When is it safe to ignore this mismatch?

**Answer:** When the old policy and new policy are close.

---

## Bounding the Error

If the policies $\pi_\theta$ and $\pi_{\theta'}$ are close, the state distributions $p_\theta$ and $p_{\theta'}$ are also close.

We can bound the difference in state distributions using the **KL Divergence** between the policies:

$$| p_{\theta'}(s) - p_\theta(s) | \le 2 \epsilon t$$

where $\epsilon \approx \max_s D_{KL}(\pi_{\theta'}(\cdot|s) | \pi_{\theta}(\cdot|s))$.

This leads to the **Monotonic Improvement Bound:**

We can maximize a "surrogate" objective (the one that ignores state distribution changes), and if we keep the KL divergence small, we are guaranteed to improve the true objective $J(\theta)$.

---

## Trust Region Policy Optimization (TRPO)

This theoretical bound suggests a constrained optimization problem:

**Objective (Surrogate):**

$$\theta_{new} = \arg\max_{\theta'} \mathbb{E}_{s \sim \rho_{\pi_{old}}} \left[ \frac{\pi_{\theta'}(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{old}}(s,a) \right]$$

**Constraint (Trust Region):**

$$D_{KL}(\pi_{\theta_{old}}(\cdot|s) | \pi_{\theta'}(\cdot|s)) \le \epsilon$$

We want to jump to the best possible policy within a "safe" region where our local approximation holds.

---

## Natural Policy Gradient

How do we solve that constrained optimization problem efficiently?

1. **Linearize the Objective:** Approximate the objective with the first-order gradient $\nabla_\theta J(\theta)$.
2. **Quadratic Constraint:** Approximate the KL divergence constraint using the second-order Taylor expansion (the Fisher Information Matrix, $F$).

$$D_{KL}(\pi_{\theta'} | \pi_\theta) \approx \frac{1}{2} (\theta' - \theta)^T F (\theta' - \theta)$$

where $F$ is the **Fisher Information Matrix**:

$$F = \mathbb{E}_{\pi_\theta} \left[ \nabla \log \pi(a|s) \nabla \log \pi(a|s)^T \right]$$

### The Optimization Landscape

Standard gradient ascent ($\theta \leftarrow \theta + \alpha \nabla J$) moves in Euclidean parameter space. This is bad because some parameters change the policy probability distribution much more than others.

We want to move in **distribution space**. The direction that points "steepest ascent" in distribution space is not $\nabla J$, but $F^{-1} \nabla J$.

### Natural Gradient Update

$$\theta_{new} = \theta_{old} + \alpha F^{-1} \nabla_\theta J(\theta)$$

where the step size $\alpha$ is determined analytically by the KL constraint $\epsilon$:

$$\alpha = \sqrt{\frac{2\epsilon}{\nabla J^T F^{-1} \nabla J}}$$

### Why This is Better

- **Curvature:** $F$ measures sensitivity. If changing a weight changes the policy output a lot, $F$ is large, so $F^{-1}$ makes the step small. If the policy is insensitive, the step is large.
- **Invariance:** It is invariant to reparameterization (it cares about the policy output, not how the weights are stored).
- **Avoids Plateaus:** It escapes "flat" regions of the objective landscape faster than standard gradient descent.

---

## Summary of Methods

|Method|Objective|Constraint/Step|Note|
|---|---|---|---|
|Vanilla PG|Maximize $J$|Fixed step size $\alpha$|Often unstable; step size is hard to tune.|
|Natural PG|Maximize $J$|Fixed KL radius|Rescales gradient by inverse Fisher Matrix ($F^{-1}$).|
|TRPO|Maximize Surrogate|Hard KL constraint|Solves constrained problem using Conjugate Gradient (approximates $F^{-1} \nabla J$).|
|PPO|Maximize Surrogate|Soft KL penalty / Clipping|Simpler, first-order approximation of TRPO (very popular).|

---

## Key Takeaway

Regular gradient ascent has the wrong geometry for probability distributions. Natural Gradient (and approximations like TRPO/PPO) correct this by ensuring the policy changes smoothly, not just the parameters.